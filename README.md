# An Implementation of the DS-STAR Agent System

This is an implementation of the DS-STAR Agent system by Google Cloud in LangGraph.

The following sections have been taken as is from the original paper: [https://arxiv.org/pdf/2509.21825](https://arxiv.org/pdf/2509.21825)

---

## 3. DS-STAR

In this section, we introduce DS-STAR, a framework for data science agents that leverages the coding and reasoning capabilities of LLMs to tackle data science tasks. In a nutshell, our approach first analyzes heterogeneous input data files to extract essential information (Section 3.1). Subsequently, DS-STAR generates a code solution through an iterative process (Section 3.2). The prompts and algorithms are detailed in Appendix G and A, respectively.

### Problem Setup

Our goal is to automatically generate a code solution $s$ (*i.e.*, a Python script) that answers query $q$ using a given data files $\mathcal{D}$. We formulate this as a search problem over the space of all possible scripts, denoted by $\mathcal{S}$. The quality of any script $s \in \mathcal{S}$ is evaluated by a scoring function $h(s)$, which measures the correctness of its output, $s(\mathcal{D})$, against a ground-truth answer (*e.g.*, accuracy) where $s(\mathcal{D})$ represents the output generated by executing $s$, which loads the data $\mathcal{D}$ to produce an answer. The objective is to identify the $s^*$ that is:

$$s^* = \arg\max_{s \in \mathcal{S}} h(s(\mathcal{D}))$$

In this paper, we propose a multi-agent framework, $\mathcal{A}$, designed to process the query $q$ and data files $\mathcal{D}$, which may be numerous and in heterogeneous formats. This framework is composed of $n$ specialized LLM agents, $\\{\mathcal{A}_i\\}_{i=1}^n$, where each agent possesses a distinct functionality, as detailed in the subsequent sections (see Figure 2 for the problem setup and our agent's overview).

---

## 3.1. Analyzing Data Files

To effectively interact with the given data files, DS-STAR first requires a comprehensive understanding of its contents and structure. We achieve this by generating a concise, analytical description for each data file. This initial analysis phase is critical as it informs all subsequent actions taken by the agent.

Conventional frameworks for data science agents often rely on displaying a few sample rows from structured files, such as CSVs. However, this approach is fundamentally limited to structured data and fails for unstructured formats where the concept of a 'row' is ill-defined. To overcome this limitation, we introduce a more general mechanism as follows. For each data file $\mathcal{D}_i \in \mathcal{D}$ we employ an analyzer agent, $\mathcal{A}_{\text{analyzer}}$, to generate a Python script, $s_{\text{desc}}^i$. This script is designed to correctly parse and load the data file $\mathcal{D}_i$ and then extract its essential properties. For structured data, this might include column names and data types; for unstructured data, it could be file metadata, text summaries. The resulting analytical description, $d_i$, is captured directly from the script's execution output (see Appendix D). This process, which can be parallelized, is denoted as:

$$d_i = \text{exec}(s_{\text{desc}}^i), \quad s_{\text{desc}}^i = \mathcal{A}_{\text{analyzer}}(\mathcal{D}_i)$$

---

## 3.2. Iterative Plan Generation and Verification

### Initialization

After generating analytic descriptions of the $N$ data files, DS-STAR begins the solution generation process. First, a planner agent, $\mathcal{A}_{\text{planner}}$, generates an initial high-level executable step $p_0$ (*e.g.*, loading a data file) using the query $q$ and obtained data descriptions:

$$p_0 = \mathcal{A}_{\text{planner}}(q, \\{d_i\\}_{i=1}^N)$$

This single executable step is then implemented as a code script $s_0$ by a coder agent $\mathcal{A}_{\text{coder}}$ and the initial execution result $r_0$ is then obtained by executing the script $s_0$ as follows:

$$s_0 = \mathcal{A}_{\text{coder}}(p_0, \\{d_i\\}_{i=1}^N), \quad r_0 = \text{exec}(s_0) \tag{1}$$

### Plan Verification

The main challenge in data science tasks is guiding the refinement of a solution, as determining its correctness is often non-trivial, since there are no ground-truth label. To address this, we use an LLM as a judge to assess whether the current plan is sufficient for the user's query.

Our approach introduces a verifier agent $\mathcal{A}_{\text{verifier}}$. At any given round $k$ in the problem-solving process, this agent evaluates the state of the solution, *i.e.*, whether the plan is sufficient to solve the problem. The evaluation is based on the cumulative plan $p = \\{p_0, \cdots, p_k\\}$, the user's query $q$, the current solution code $s_k$, which is an implementation of the cumulative plan, and its execution result $r_k$. The operation of $\mathcal{A}_{\text{verifier}}$ is denoted as follows:

$$v = \mathcal{A}_{\text{verifier}}(p, q, s_k, r_k)$$

Here, the output $v$ is a binary variable: sufficient or insufficient. Note that our method does not just compare the plan to the query. By conditioning the judgement on $s_k$ and its execution output $r_k$, $\mathcal{A}_{\text{verifier}}$ can provide more grounded feedback since it assesses whether $s_k$ is well-implemented following the plan, and whether the $r_k$ contains the information needed to fully address the query.

### Plan Refinement

If the verifier agent $\mathcal{A}_{\text{verifier}}$ determines that the current plan is insufficient to solve the user's query $q$, DS-STAR must decide how to proceed. Such insufficiency could arise because the plan is merely incomplete and requires additional steps, or because it contains erroneous steps that invalidate the approach. To resolve this, DS-STAR employs a router agent, $\mathcal{A}_{\text{router}}$, which decides whether to append a new step or to correct an existing one. The router's decision $w$ is generated as follows, where $p = \\{p_0, \cdots, p_k\\}$ is the current cumulative plan:

$$w = \mathcal{A}_{\text{router}}(p, q, \\{d_i\\}_{i=1}^N)$$

The output $w$ is either the token **Add Step** or an index $l \in \\{1, \cdots, k\\}$. If $w$ is **Add Step**, $\mathcal{A}_{\text{router}}$ has determined the plan is correct but incomplete. In this case, we retain the plan $p$ and proceed to generate the next step. On the other hand, if $w = l$, $\mathcal{A}_{\text{router}}$ has identified $p_l$ as erroneous. In this case, we backtrack by truncating the plan to $p \leftarrow \\{p_0, \cdots, p_{l-1}\\}$. Here, we deliberately choose to truncate and regenerate through the LLM's random sampling, rather than directly correcting $p_l$, since our empirical finding has shown that revising a specific incorrect step often leads to an overly complex replacement, therefore frequently flagged again by $\mathcal{A}_{\text{router}}$ in a next iteration. Following the decision from the $\mathcal{A}_{\text{router}}$, our agent proceeds with an updated plan $p = \\{p_0, \cdots, p_{k'}\\}$, where $k' = k$ or $k' = l - 1$. Then DS-STAR generates a subsequent step:

$$p_{k'+1} = \mathcal{A}_{\text{planner}}(p, q, r_k, \\{d_i\\}_{i=1}^N)$$

Notably, the planner agent $\mathcal{A}_{\text{planner}}$ is conditioned on the last execution result, $r_k$, enabling it to generate a step that attempts to resolve the previously identified insufficiency. Once the new step $p_{k'+1}$ is defined, the plan is updated to:

$$p \leftarrow \\{p_0, \cdots, p_{k'}, p_{k'+1}\\} \tag{2}$$

### Plan Implementation and Execution

Finally, DS-STAR enters an execution and verification cycle. First, $\mathcal{A}_{\text{coder}}$ implements $p$ into code $s$. The execution of this code yields a new observation $r = \text{exec}(s)$. With this $r$, $\mathcal{A}_{\text{verifier}}$ is invoked again to assess if the newly augmented plan is now sufficient. This entire iterative procedure—routing, planning, coding, executing, and verifying—is repeated until $\mathcal{A}_{\text{verifier}}$ returns a sufficient or a predefined maximum number of iterations is reached.

---

## 3.3. Additional Modules for Robust Data Science Agents

### Debugging Agent

When a Python script $s$ fails during execution, it generates an error traceback $\mathcal{T}_{\text{bug}}$. To automatically debug the script, DS-STAR employs a debugging agent, $\mathcal{A}_{\text{debugger}}$. First, when generating $\\{d_i\\}_{i=1}^N$ using $s_{\text{desc}}$ obtained from $\mathcal{A}_{\text{analyzer}}$ (see Section 3.1), $\mathcal{A}_{\text{debugger}}$ iteratively update the script using only the traceback:

$$s_{\text{desc}} \leftarrow \mathcal{A}_{\text{debugger}}(s_{\text{desc}}, \mathcal{T}_{\text{bug}})$$

Secondly, once DS-STAR obtains $\\{d_i\\}_{i=1}^N$, $\mathcal{A}_{\text{debugger}}$ utilizes such information when generating a solution Python script $s$ (see Section 3.2). Our key insight is that tracebacks alone are often insufficient for resolving errors in data-centric scripts, while $\\{d_i\\}_{i=1}^N$ might include critical metadata such as column headers in a CSV file, sheet names in an Excel workbook, or database schema information. Therefore, $\mathcal{A}_{\text{debugger}}$ generates a corrected script, $s$, by conditioning on the original script $s$, the error traceback $\mathcal{T}_{\text{bug}}$, and this rich data context $\\{d_i\\}_{i=1}^N$:

$$s \leftarrow \mathcal{A}_{\text{debugger}}(s, \mathcal{T}_{\text{bug}}, \\{d_i\\}_{i=1}^N)$$

### Retriever

A potential scalability challenge arises when the number of data files $N$ is large (*i.e.*, $N > 100$). In such cases, the complete set of descriptions $\\{d_i\\}_{i=1}^N$ cannot be prompted within the predefined context length of LLMs. To address this limitation, we employ a retrieval mechanism that leverages a pre-trained embedding model (Nie et al., 2024). Specifically, we identify the top-$K$ most relevant data files, which will be provided as context to the LLM, by computing the cosine similarity between the embedding of the user's query $q$ and the embedding of each description $d_i$.

---

## 3.4. LangGraph Implementation

The reference implementation in `ds_star.py` encodes Algorithm 1 as a LangGraph state machine. The graph maintains a typed state that includes the user query, candidate plan steps, intermediate scripts, execution traces, and bookkeeping metadata (iteration counts, router decision, finalization reason, etc.). Each node in the graph corresponds to one of the specialized agents:

- **`analyze`** — invokes the analyzer agent on every input file and uses the analyzer debugger (plus optional traceback summarization) for iterative self-correction.
- **`planner_initial`** / **`planner_next`** — call the planner agent to seed and refine the plan while conditioning on the latest execution observation.
- **`coder_initial`** / **`coder_next`** — generate executable Python via the coder agent and persist the running script inside the graph state.
- **`execute`** — runs the current script with guarded subprocess execution and optional solution debugging.
- **`verify`** — queries the verifier agent to determine sufficiency and advances the iteration counter.
- **`router`** — decides whether to append a fresh plan step or truncate to an earlier step.
- **`finalize`** — invokes the finalyzer agent (if configured) to format the deliverable solution.

Each agent lives in its own module under `ds_star_agents/`, enabling independent prompt management and easier customization. Shared formatting helpers and prompt-loading utilities are provided in `ds_star_core/utils.py`. The resulting graph loops through the `execute → verify → router → planner_next → coder_next` cycle until the verifier accepts the plan or the maximum number of refinement rounds is exceeded, at which point the final plan, script, and execution log are returned.

## Implementation Notes

- The coordinator in `ds_star.py` builds every agent instance through `AgentBundle`, mirroring the roles defined in Section 3 of the paper.
- Analyzer scripts are produced and debugged by `AnalyzerService`, which executes each script and retries with traceback-guided fixes, aligning with Equation (29).
- Iterative execution uses `SolutionExecutionService`, `PlanningService`, and `CodingService` so that the `execute → verify → router` loop matches Algorithm&nbsp;1 exactly.
- Router decisions expect either the literal token `Add Step` or an integer `l`; when an integer is returned, the plan is truncated to `{p_0, …, p_{l-1}}` before generating a new step, as described in Section 3.2.
- The verifier response is interpreted strictly using the `sufficient` / `insufficient` vocabulary specified in the paper, ensuring the graph halts precisely when the spec dictates.

---

## Algorithm 1: DS-STAR

```
Input: query q, data files D, maximum number of refinement round M

# Analyzing data files
for i = 1 to N do
    s_desc^i = A_analyzer(D_i)
    d_i = exec(s_desc^i)
end for

# Iterative plan generation and verification
p_0 = A_planner(q, {d_i}_{i=1}^N)
s_0 = A_coder(p_0, {d_i}_{i=1}^N)
r_0 = exec(s_0)
p = {p_0}

for k = 0 to M - 1 do
    v = A_verifier(p, q, s_k, r_k)
    
    if v = sufficient then
        break
    else if v = insufficient then
        w = A_router(p, q, r_k, {d_i}_{i=1}^N)
        
        if w ∈ {0, ..., len(p) - 1} then
            l = w - 1
        else if w = Add Step then
            l = k
        end if
        
        p ← {p_0, ..., p_l}
        p_{l+1} = A_planner(p, q, r_k, {d_i}_{i=1}^N)
        p ← {p_0, ..., p_l, p_{l+1}}
        s_{k+1} = A_coder(p, q, s_k, {d_i}_{i=1}^N)
        r_{k+1} = exec(s_{k+1})
    end if
end for

Output: Final solution s
```

---

## Reference

**Paper:** [DS-STAR:  Data Science Agent via Iterative
Planning and Verification](https://arxiv.org/pdf/2509.21825)  
**Authors:** Jaehyun Nam (1,2), Jinsung Yoon (1), Jiefeng Chen (1) and Tomas Pfister (1) (Google Cloud, KAIST)

---

## License

I've implemented this myself and I allow you to do whatever you want with the code itself, just remember to cite the original paper referenced above.