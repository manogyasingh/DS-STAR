This is an implementation of the DS-STAR Agent system by Google DeepMind in LangGraph.

The following section has been taken as is from the original paper https://arxiv.org/pdf/2509.21825

# 3. DS-STAR

In this section, we introduce DS-STAR, a framework for data science agents that leverages the coding and reasoning capabilities of LLMs to tackle data science tasks. In a nutshell, our approach first analyzes heterogeneous input data files to extract essential information (Section 3.1). Subsequently, DS-STAR generates a code solution through an iterative process (Section 3.2). The prompts and algorithms are detailed in Appendix G and A, respectively.

**Problem setup.** Our goal is to automatically generate a code solution $s$ (*i.e.*, a Python script) that answers query $q$ using a given data files $\mathcal{D}$. We formulate this as a search problem over the space of all possible scripts, denoted by $\mathcal{S}$. The quality of any script $s \in \mathcal{S}$ is evaluated by a scoring function $h(s)$, which measures the correctness of its output, $s(\mathcal{D})$, against a ground-truth answer (*e.g.*, accuracy) where $s(\mathcal{D})$ represents the output generated by executing $s$, which loads the data $\mathcal{D}$ to produce an answer. The objective is to identify the $s^*$ that is: $s^* = \arg\max_{s \in \mathcal{S}} h(s(\mathcal{D}))$.

In this paper, we propose a multi-agent framework, $\mathcal{A}$, designed to process the query $q$ and data files $\mathcal{D}$, which may be numerous and in heterogeneous formats. This framework is composed of $n$ specialized LLM agents, $\{\mathcal{A}_i\}_{i=1}^n$, where each agent possesses a distinct functionality, as detailed in the subsequent sections (see Figure 2 for the problem setup and our agent's overview).

## 3.1. Analyzing data files

To effectively interact with the given data files, DS-STAR first requires a comprehensive understanding of its contents and structure. We achieve this by generating a concise, analytical description for each data file. This initial analysis phase is critical as it informs all subsequent actions taken by the agent.

Conventional frameworks for data science agents often rely on displaying a few sample rows from structured files, such as CSVs. However, this approach is fundamentally limited to structured data and fails for unstructured formats where the concept of a 'row' is ill-defined. To overcome this limitation, we introduce a more general mechanism as follows. For each data file $\mathcal{D}_i \in \mathcal{D}$ we employ an analyzer agent, $\mathcal{A}_{\text{analyzer}}$, to generate a Python script, $s_{\text{desc}}^i$. This script is designed to correctly parse and load the data file $\mathcal{D}_i$ and then extract its essential properties. For structured data, this might include column names and data types; for unstructured data, it could be file metadata, text summaries. The resulting analytical description, $d_i$, is captured directly from the script's execution output (see Appendix D). This process, which can be parallelized, is denoted as: $d_i = \text{exec}(s_{\text{desc}}^i), s_{\text{desc}}^i = \mathcal{A}_{\text{analyzer}}(\mathcal{D}_i)$.

## 3.2. Iterative plan generation and verification

**Initialization.** After generating analytic descriptions of the $N$ data files, DS-STAR begins the solution generation process. First, a planner agent, $\mathcal{A}_{\text{planner}}$, generates an initial high-level executable step $p_0$ (*e.g.*, loading a data file) using the query $q$ and obtained data descriptions: $p_0 = \mathcal{A}_{\text{planner}}(q, \{d_i\}_{i=1}^N)$.

This single executable step is then implemented as a code script $s_0$ by a coder agent $\mathcal{A}_{\text{coder}}$ and the initial execution result $r_0$ is then obtained by executing the script $s_0$ as follows:

$$s_0 = \mathcal{A}_{\text{coder}}(p_0, \{d_i\}_{i=1}^N), \quad r_0 = \text{exec}(s_0). \tag{1}$$

**Plan verification.** The main challenge in data science tasks is guiding the refinement of a solution, as determining its correctness is often non-trivial, since there are no ground-truth label. To address this, we use an LLM as a judge to assess whether the current plan is sufficient for the user's query.

Our approach introduces a verifier agent $\mathcal{A}_{\text{verifier}}$. At any given round $k$ in the problem-solving process, this agent evaluates the state of the solution, *i.e.*, whether the plan is sufficient to solve the problem. The evaluation is based on the cumulative plan $p = \{p_0, \cdots, p_k\}$, the user's query $q$, the current solution code $s_k$, which is an implementation of the cumulative plan, and its execution result $r_k$. The operation of $\mathcal{A}_{\text{verifier}}$ is denoted as follows: $v = \mathcal{A}_{\text{verifier}}(p, q, s_k, r_k)$.

Here, the output $v$ is a binary variable: sufficient or insufficient. Note that our method does not just compare the plan to the query. By conditioning the judgement on $s_k$ and its execution output $r_k$, $\mathcal{A}_{\text{verifier}}$ can provide more grounded feedback since it assesses whether $s_k$ is well-implemented following the plan, and whether the $r_k$ contains the information needed to fully address the query.

**Plan refinement.** If the verifier agent $\mathcal{A}_{\text{verifier}}$ determines that the current plan is insufficient to solve the user's query $q$, DS-STAR must decide how to proceed. Such insufficiency could arise because the plan is merely incomplete and requires additional steps, or because it contains erroneous steps that invalidate the approach. To resolve this, DS-STAR employs a router agent, $\mathcal{A}_{\text{router}}$, which decides whether to append a new step or to correct an existing one. The router's decision $w$ is generated as follows, where $p = \{p_0, \cdots, p_k\}$ is the current cumulative plan: $w = \mathcal{A}_{\text{router}}(p, q, \{d_i\}_{i=1}^N)$.

The output $w$ is either the token Add Step or an index $l \in \{1, \cdots, k\}$. If $w$ is Add Step, $\mathcal{A}_{\text{router}}$ has determined the plan is correct but incomplete. In this case, we retain the plan $p$ and proceed to generate the next step. On the other hand, if $w = l$, $\mathcal{A}_{\text{router}}$ has identified $p_l$ as erroneous. In this case, we backtrack by truncating the plan to $p \leftarrow \{p_0, \cdots, p_{l-1}\}$. Here, we deliberately choose to truncate and regenerate through the LLM's random sampling, rather than directly correcting $p_l$, since our empirical finding has shown that revising a specific incorrect step often leads to an overly complex replacement, therefore frequently flagged again by $\mathcal{A}_{\text{router}}$ in a next iteration. Following the decision from the $\mathcal{A}_{\text{router}}$, our agent proceeds with an updated plan $p = \{p_0, \cdots, p_{k'}\}$, where $k' = k$ or $k' = l - 1$. Then DS-STAR generates a subsequent step: $p_{k'+1} = \mathcal{A}_{\text{planner}}(p, q, r_k, \{d_i\}_{i=1}^N)$.

Notably, the planner agent $\mathcal{A}_{\text{planner}}$ is conditioned on the last execution result, $r_k$, enabling it to generate a step that attempts to resolve the previously identified insufficiency. Once the new step $p_{k'+1}$ is defined, the plan is updated to:

$$p \leftarrow \{p_0, \cdots, p_{k'}, p_{k'+1}\}. \tag{2}$$

**Plan implement and execution.** Finally, DS-STAR enters an execution and verification cycle. First, $\mathcal{A}_{\text{coder}}$ implements $p$ into code $s$. The execution of this code yields a new observation $r = \text{exec}(s)$. With this $r$, $\mathcal{A}_{\text{verifier}}$ is invoked again to assess if the newly augmented plan is now sufficient. This entire iterative procedure—routing, planning, coding, executing, and verifying—is repeated until $\mathcal{A}_{\text{verifier}}$ returns a sufficient or a predefined maximum number of iterations is reached.

## 3.3. Additional modules for robust data science agents

**Debugging agent.** When a Python script $s$ fails during execution, it generates an error traceback $\mathcal{T}_{\text{bug}}$. To automatically debug the script, DS-STAR employs a debugging agent, $\mathcal{A}_{\text{debugger}}$. First, when generating $\{d_i\}_{i=1}^N$ using $s_{\text{desc}}$ obtained from $\mathcal{A}_{\text{analyzer}}$ (see Section 3.1), $\mathcal{A}_{\text{debugger}}$ iteratively update the script using only the traceback: $s_{\text{desc}} \leftarrow \mathcal{A}_{\text{debugger}}(s_{\text{desc}}, \mathcal{T}_{\text{bug}})$.

Secondly, once DS-STAR obtains $\{d_i\}_{i=1}^N$, $\mathcal{A}_{\text{debugger}}$ utilizes such information when generating a solution Python script $s$ (see Section 3.2). Our key insight is that tracebacks alone are often insufficient for resolving errors in data-centric scripts, while $\{d_i\}_{i=1}^N$ might include critical metadata such as column headers in a CSV file, sheet names in an Excel workbook, or database schema information. Therefore, $\mathcal{A}_{\text{debugger}}$ generates a corrected script, $s$, by conditioning on the original script $s$, the error traceback $\mathcal{T}_{\text{bug}}$, and this rich data context $\{d_i\}_{i=1}^N$: $s \leftarrow \mathcal{A}_{\text{debugger}}(s, \mathcal{T}_{\text{bug}}, \{d_i\}_{i=1}^N)$.

**Retriever.** A potential scalability challenge arises when the number of data files $N$ is large (*i.e.*, $N > 100$.). In such cases, the complete set of descriptions $\{d_i\}_{i=1}^N$ cannot be prompted within the predefined context length of LLMs. To address this limitation, we employ a retrieval mechanism that leverages a pre-trained embedding model (Nie et al., 2024). Specifically, we identify the top-$K$ most relevant data files, which will be provided as context to the LLM, by computing the cosine similarity between the embedding of the user's query $q$ and the embedding of each description $d_i$.

# 3.4. LangGraph implementation

The reference implementation in `ds_star.py` encodes Algorithm&nbsp;1 as a LangGraph state machine. The graph maintains a typed state that includes the user query, candidate plan steps, intermediate scripts, execution traces, and bookkeeping metadata (iteration counts, router decision, finalization reason, etc.). Each node in the graph corresponds to one of the specialized agents:

- `analyze` — invokes the analyzer agent on every input file and uses the analyzer debugger (plus optional traceback summarization) for iterative self-correction.
- `planner_initial` / `planner_next` — call the planner agent to seed and refine the plan while conditioning on the latest execution observation.
- `coder_initial` / `coder_next` — generate executable Python via the coder agent and persist the running script inside the graph state.
- `execute` — runs the current script with guarded subprocess execution and optional solution debugging.
- `verify` — queries the verifier agent to determine sufficiency and advances the iteration counter.
- `router` — decides whether to append a fresh plan step or truncate to an earlier step.
- `finalize` — invokes the finalyzer agent (if configured) to format the deliverable solution.

Each agent lives in its own module under `ds_star_agents/`, enabling independent prompt management and easier customization. Shared formatting helpers and prompt-loading utilities are provided in `ds_star_core/utils.py`. The resulting graph loops through the `execute → verify → router → planner_next → coder_next` cycle until the verifier accepts the plan or the maximum number of refinement rounds is exceeded, at which point the final plan, script, and execution log are returned.

# Algorithm 1 DS-STAR

1: **Input:** query $q$, data files $\mathcal{D}$, maximum number of refinement round $M$

2: # Analyzing data files

3: **for** $i = 1$ to $N$ **do**

4: $\quad s_{\text{desc}}^i = \mathcal{A}_{\text{analyzer}}(\mathcal{D}_i)$

5: $\quad d_i = \text{exec}(s_{\text{desc}}^i)$

6: **end for**

7: # Iterative plan generation and verification

8: $p_0 = \mathcal{A}_{\text{planner}}(q, \{d_i\}_{i=1}^N)$

9: $s_0 = \mathcal{A}_{\text{coder}}(p_0, \{d_i\}_{i=1}^N)$

10: $r_0 = \text{exec}(s_0)$

11: $p = \{p_0\}$

12: **for** $k = 0$ to $M - 1$ **do**

13: $\quad v = \mathcal{A}_{\text{verifier}}(p, q, s_k, r_k)$

14: $\quad$ **if** $v = \text{sufficient}$ **then**

15: $\quad\quad$ **break**

16: $\quad$ **else if** $v = \text{insufficient}$ **then**

17: $\quad\quad w = \mathcal{A}_{\text{router}}(p, q, r_k, \{d_i\}_{i=1}^N)$

18: $\quad\quad$ **if** $w \in \{0, \cdots, \text{len}(p) - 1\}$ **then**

19: $\quad\quad\quad l = w - 1$

20: $\quad\quad$ **else if** $w = \text{Add Step}$ **then**

21: $\quad\quad\quad l = k$

22: $\quad\quad$ **end if**

23: $\quad\quad p \leftarrow \{p_0, \cdots, p_l\}$

24: $\quad\quad p_{l+1} = \mathcal{A}_{\text{planner}}(p, q, r_k, \{d_i\}_{i=1}^N)$

25: $\quad\quad p \leftarrow \{p_0, \cdots, p_l, p_{l+1}\}$

26: $\quad\quad s_{k+1} = \mathcal{A}_{\text{coder}}(p, q, s_k, \{d_i\}_{i=1}^N)$

27: $\quad\quad r_{k+1} = \text{exec}(s_{k+1})$

28: $\quad$ **end if**

29: **end for**

30: **Output:** Final solution $s$